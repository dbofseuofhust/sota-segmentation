##+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
## Created by: speedinghzl02
## Modified by: RainbowSecret
## Microsoft Research
## yuyua@microsoft.com
## Copyright (c) 2018
##
## This source code is licensed under the MIT-style license found in the
## LICENSE file in the root directory of this source tree 
##+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

import argparse
import torch
import torch.nn as nn
from torch.utils import data
import numpy as np
import pickle
import cv2
from torch.autograd import Variable
import torch.optim as optim
import scipy.misc
import torch.backends.cudnn as cudnn
import sys
import os
import os.path as osp
from torch.nn.parallel.scatter_gather import gather
from encoding.datasets import get_segmentation_dataset
from encoding.models import get_segmentation_model
from option import Parameters
import torchvision.transforms as transform
import random
import timeit
import logging
import pdb
from tqdm import tqdm
import encoding.utils as utils
from tensorboardX import SummaryWriter
from encoding.nn import CriterionCrossEntropy,  CriterionDSN, CriterionOhemDSN, CriterionOhemDSN_single
from encoding.models import DataParallelModel, DataParallelCriterion

torch_ver = torch.__version__[:3]
if torch_ver == '0.3':
    from torch.autograd import Variable

start = timeit.default_timer()
args = Parameters().parse()

def lr_poly(base_lr, epoch, epochs, power):
    return base_lr*((1-float(epoch)/epochs)**(power))

def adjust_learning_rate(optimizer, epoch):
    lr = lr_poly(args.lr, epoch, args.epochs, args.power)
    optimizer.param_groups[0]['lr'] = lr
    return lr

def validation(args,model,valloader,optimizer,epoch,best_pred,nclass):
    # Fast test during the training
    def eval_batch(model, image, target):
        outputs = model(image)
        outputs = gather(outputs, 0, dim=0)
        pred = outputs[0]
        target = target.cuda()
        correct, labeled = utils.batch_pix_accuracy(pred.data, target)
        inter, union = utils.batch_intersection_union(pred.data, target, nclass)
        return correct, labeled, inter, union

    is_best = False
    model.eval()
    total_inter, total_union, total_correct, total_label = 0, 0, 0, 0
    tbar = tqdm(valloader, desc='\r')

    for i, (image, target) in enumerate(tbar):
        if torch_ver == "0.3":
            image = Variable(image, volatile=True)
            correct, labeled, inter, union = eval_batch(model, image, target)
        else:
            with torch.no_grad():
                correct, labeled, inter, union = eval_batch(model, image, target)

        total_correct += correct
        total_label += labeled
        total_inter += inter
        total_union += union
        pixAcc = 1.0 * total_correct / (np.spacing(1) + total_label)
        IoU = 1.0 * total_inter / (np.spacing(1) + total_union)
        mIoU = IoU.mean()
        tbar.set_description(
                'pixAcc: %.3f, mIoU: %.3f' % (pixAcc, mIoU))

    new_pred = (pixAcc + mIoU) / 2
    if new_pred > best_pred:
        is_best = True
        best_pred = new_pred
        utils.save_checkpoint({
            'epoch': epoch + 1,
            'state_dict': model.module.state_dict(),
            'optimizer': optimizer.state_dict(),
            'best_pred': best_pred,
        }, args, is_best)
        return best_pred

def main():
    print("Input arguments:")
    for key, val in vars(args).items():
        print("{:16} {}".format(key, val))

    random.seed(args.seed)
    torch.manual_seed(args.seed)

    args.log_name = str(args.checkname)
    logger = utils.create_logger(args.log_root, args.log_name)

    writer = SummaryWriter(args.snapshot_dir)
    os.environ["CUDA_VISIBLE_DEVICES"]=args.gpu
    cudnn.enabled = True

    deeplab = get_segmentation_model(args.model, dataset=args.dataset)

    logger.info(deeplab)

    model = DataParallelModel(deeplab)
    model.train()
    model.float()
    model.cuda()    

    criterion = CriterionCrossEntropy()
    if "dsn" in args.model:
        if args.ohem:
            if args.ohem_single:
                print('use ohem only for the second prediction map.')
                criterion = CriterionOhemDSN_single(thres=args.ohem_thres, min_kept=args.ohem_keep, dsn_weight=float(args.dsn_weight))
            else:
                criterion = CriterionOhemDSN(thres=args.ohem_thres, min_kept=args.ohem_keep, dsn_weight=float(args.dsn_weight), use_weight=True)
        else:
            criterion = CriterionDSN(dsn_weight=float(args.dsn_weight), use_weight=True)

    criterion = DataParallelCriterion(criterion)
    criterion.cuda()
    cudnn.benchmark = True

    if not os.path.exists(args.snapshot_dir):
        os.makedirs(args.snapshot_dir)

    trainset = get_segmentation_dataset(args.dataset, split='train', mode='train', crop_size=(args.crop_size,args.crop_size),
                                        scale=args.scale, mirror=True, ignore_label=255, use_aug=False)
    testset = get_segmentation_dataset(args.dataset, split='val', mode='val', crop_size=(args.crop_size,args.crop_size),
                                        scale=args.scale, mirror=False, ignore_label=255, use_aug=False)

    nclass = trainset.NUM_CLASS

    # dataloader
    kwargs = {'num_workers': args.workers, 'pin_memory': True}
    trainloader = data.DataLoader(trainset, batch_size=args.batch_size,
                                       drop_last=True, shuffle=True, **kwargs)
    valloader = data.DataLoader(testset, batch_size=args.batch_size,
                                     drop_last=False, shuffle=False, **kwargs)

    optimizer = optim.SGD([{'params': filter(lambda p: p.requires_grad, deeplab.parameters()), 'lr': args.lr}],
                lr=args.lr, momentum=args.momentum, weight_decay=args.weight_decay)

    optimizer.zero_grad()

    best_pred = 0

    for epoch in range(args.start_epoch, args.epochs):
        train_loss = 0.0
        model.train()
        tbar = tqdm(trainloader)
        for i_iter, batch in enumerate(tbar):
            sys.stdout.flush()
            images, labels,_,_ = batch
            images = Variable(images.cuda())
            labels = Variable(labels.long().cuda())
            optimizer.zero_grad()
            lr = adjust_learning_rate(optimizer, i_iter)
            if args.fix_lr:
                lr = args.learning_rate
            print('learning_rate: {}'.format(lr))

            preds = model(images)
            loss = criterion(preds, labels, **kwargs)
            loss.backward()
            optimizer.step()
            train_loss += loss.item()
            tbar.set_description('Train loss: %.3f' % (train_loss / (i_iter + 1)))
            logger.info('Train loss: %.3f' % (train_loss / (i_iter + 1)))

            if i_iter % 100 == 0:
                writer.add_scalar('learning_rate', lr, i_iter)
                writer.add_scalar('loss', loss.data.cpu().numpy(), i_iter)

        if not args.no_val:
            best_pred = validation(args=args,model=model,valloader=valloader,optimizer=optimizer,epoch=epoch,best_pred=best_pred,nclass=nclass)

    end = timeit.default_timer()
    print(end-start,'seconds')

if __name__ == '__main__':
    main()
